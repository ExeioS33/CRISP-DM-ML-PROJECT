{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267e0933-eb83-49d2-80ae-fd0025ce94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490f812-5f7c-4416-9e19-6d76f31580e4",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989ddd31-c5d3-4691-8bd3-bcd1e260103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_properties(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extracts general properties of the dataset.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Number of Rows\": data.shape[0],\n",
    "        \"Number of Columns\": data.shape[1],\n",
    "        \"Columns\": list(data.columns),\n",
    "        \"Missing Values\": data.isnull().sum().to_dict(),\n",
    "        \"Duplicated Rows\": data.duplicated().sum(),\n",
    "        \"Data Types\": data.dtypes.to_dict(),\n",
    "    }\n",
    "\n",
    "\n",
    "def describe_numerical(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Describes numerical variables in the dataset.\n",
    "    \"\"\"\n",
    "    numerical_vars = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    return data[numerical_vars].describe().to_dict()\n",
    "\n",
    "\n",
    "def save_catalog_as_tsv(catalog: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Saves the data catalog as a TSV file with improved readability.\n",
    "\n",
    "    Parameters:\n",
    "    catalog (dict): The dictionary containing dataset properties.\n",
    "    file_path (str): The path where the TSV file will be saved.\n",
    "    \"\"\"\n",
    "    flattened_catalog = []\n",
    "\n",
    "    for key, value in catalog.items():\n",
    "        if isinstance(value, dict):  # Handle nested dictionaries\n",
    "            for sub_key, sub_value in value.items():\n",
    "                flattened_catalog.append((f\"{key} - {sub_key}\", sub_value))\n",
    "        elif isinstance(value, list):  # Handle lists (like column names)\n",
    "            flattened_catalog.append((key, \", \".join(map(str, value))))\n",
    "        else:\n",
    "            flattened_catalog.append((key, value))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    catalog_df = pd.DataFrame(flattened_catalog, columns=[\"Property\", \"Value\"])\n",
    "    \n",
    "    # Save as TSV\n",
    "    catalog_df.to_csv(file_path, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def describe_categorical(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Describes categorical variables in the dataset.\n",
    "    \"\"\"\n",
    "    categorical_vars = data.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    return {var: data[var].value_counts().to_dict() for var in categorical_vars}\n",
    "\n",
    "\n",
    "def get_correlation_matrix(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Computes the correlation matrix for numerical variables.\n",
    "    \"\"\"\n",
    "    numerical_vars = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    return data[numerical_vars].corr()\n",
    "\n",
    "\n",
    "def detect_outliers(data: pd.DataFrame, column: str, z_thresh: float = 3):\n",
    "    \"\"\"\n",
    "    Detects outliers in a specified column using the Z-score method.\n",
    "    \"\"\"\n",
    "    z_scores = zscore(data[column].dropna())\n",
    "    return data.loc[abs(z_scores) > z_thresh]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0186f1-917d-49dd-959b-24d2590df6e7",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8161e5-5505-4f7a-b487-e8e3b4eed510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_numerical(data: pd.DataFrame, strategy='mean'):\n",
    "    \"\"\"\n",
    "    Impute missing numerical values using specified strategy.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing numerical columns.\n",
    "    strategy (str): Imputation strategy, one of 'mean', 'median'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "    return data\n",
    "\n",
    "def standardize_numerical(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Standardize numerical features in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing numerical columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with standardized numerical features.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_numerical(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Normalize numerical features in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing numerical columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with normalized numerical features.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "    return data\n",
    "    \n",
    "    #############################################\n",
    "################## ENCODING ##########################\n",
    "    #############################################\n",
    "def one_hot_encode_categorical(data: pd.DataFrame, columns=None):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical variables in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing categorical columns.\n",
    "    columns (list): List of column names to one-hot encode. If None, encode all categorical columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with one-hot encoded categorical variables.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    return pd.get_dummies(data, columns=columns)\n",
    "\n",
    "def label_encode_categorical(data: pd.DataFrame, columns=None):\n",
    "    \"\"\"\n",
    "    Label encode categorical variables in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing categorical columns.\n",
    "    columns (list): List of column names to label encode. If None, encode all categorical columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with label encoded categorical variables.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    if columns is None:\n",
    "        columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in columns:\n",
    "        data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78236422-f5c9-4210-baaa-1f38919d7747",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d3194a-0017-4e16-ae61-cc6a07f9cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(data: pd.DataFrame, target: str):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the target variable.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x=target, data=data, palette=\"viridis\")\n",
    "    plt.title(f\"Distribution of Target Variable: {target}\")\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numerical_distributions(data: pd.DataFrame, numerical_vars: list):\n",
    "    \"\"\"\n",
    "    Plots histograms for numerical variables.\n",
    "    \"\"\"\n",
    "    data[numerical_vars].hist(\n",
    "        bins=20, figsize=(15, 10), color=\"teal\", edgecolor=\"black\"\n",
    "    )\n",
    "    plt.suptitle(\"Distributions of Numerical Variables\", size=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_categorical_distributions(data: pd.DataFrame, categorical_vars: list):\n",
    "    \"\"\"\n",
    "    Plots bar charts for categorical variables.\n",
    "    \"\"\"\n",
    "    for var in categorical_vars:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(\n",
    "            y=var, data=data, palette=\"crest\", order=data[var].value_counts().index\n",
    "        )\n",
    "        plt.title(f\"Distribution of {var}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(var)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_pairwise_relationships(\n",
    "    data: pd.DataFrame, numerical_vars: list, target: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots pairwise relationships between numerical variables.\n",
    "    \"\"\"\n",
    "    sns.pairplot(\n",
    "        data[numerical_vars + ([target] if target else [])],\n",
    "        hue=target,\n",
    "        palette=\"coolwarm\",\n",
    "    )\n",
    "    plt.suptitle(\"Pairwise Relationships\", size=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(data: pd.DataFrame, numerical_vars: list):\n",
    "    \"\"\"\n",
    "    Visualizes the correlation matrix for numerical variables.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = data[numerical_vars].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_missing_data(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualizes missing data patterns.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(data.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
    "    plt.title(\"Missing Data Heatmap\")\n",
    "    plt.xlabel(\"Columns\")\n",
    "    plt.ylabel(\"Rows\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_outlier_detection(data: pd.DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Visualizes outliers in a specified numerical variable using a boxplot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x=column, data=data, palette=\"Set2\")\n",
    "    plt.title(f\"Outlier Detection for {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91eb60-a3ea-433a-a5e0-8d52b543a848",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76bb00-41ea-4342-a0ef-6d6ec885dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data: pd.DataFrame, target: str, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame.\n",
    "    target (str): Name of the target column.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int or None): Seed for random number generation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[target])\n",
    "    y = data[target]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a machine learning model and evaluate its performance.\n",
    "\n",
    "    Parameters:\n",
    "    model: The machine learning model (already instantiated).\n",
    "    X_train (pd.DataFrame): Training features.\n",
    "    X_test (pd.DataFrame): Testing features.\n",
    "    y_train (pd.Series): Training target.\n",
    "    y_test (pd.Series): Testing target.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    return {'classification_report': report, 'confusion_matrix': matrix}\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_validate(model, X, y, cv=5, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    model: The machine learning model (already instantiated).\n",
    "    X (pd.DataFrame): Input features.\n",
    "    y (pd.Series): Target variable.\n",
    "    cv (int): Number of folds for cross-validation.\n",
    "    scoring (str or callable): Scoring metric to evaluate the model.\n",
    "\n",
    "    Returns:\n",
    "    list: Array of cross-validation scores.\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    return scores\n",
    "\n",
    "    ##################\n",
    "#### Feature Importance #####\n",
    "    ##################\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Get feature importance from a fitted model.\n",
    "\n",
    "    Parameters:\n",
    "    model: The fitted machine learning model (must have feature_importances_ or coef_ attribute).\n",
    "    feature_names (list): List of feature names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with feature names and their importance scores.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importance = model.coef_.flatten()\n",
    "    else:\n",
    "        raise ValueError(\"Model does not have attribute for feature importance.\")\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance}).sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac52cf-b772-47c0-9a4f-5038405ac87c",
   "metadata": {},
   "source": [
    "# Model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b5b1-14ac-44e2-ba21-82a22b0948a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"\n",
    "    Save a trained machine learning model to a file.\n",
    "\n",
    "    Parameters:\n",
    "    model: The trained machine learning model object.\n",
    "    filepath (str): The file path where the model will be saved.\n",
    "    \"\"\"\n",
    "    dump(model, filepath)\n",
    "    print(f\"Model saved successfully at {filepath}\")\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"\n",
    "    Load a trained machine learning model from a file.\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): The file path from which to load the model.\n",
    "\n",
    "    Returns:\n",
    "    model: The loaded machine learning model object.\n",
    "    \"\"\"\n",
    "    model = load(filepath)\n",
    "    print(f\"Model loaded successfully from {filepath}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c75492b-ddcf-4b01-996d-927b4ba1c1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     classification_report, confusion_matrix, \n\u001b[1;32m     23\u001b[0m     roc_curve, auc, precision_recall_curve, \n\u001b[1;32m     24\u001b[0m     PrecisionRecallDisplay, RocCurveDisplay\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump, load\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Generic CRISP-DM Utility Functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Union, Dict, List\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, OneHotEncoder, MinMaxScaler, \n",
    "    LabelEncoder, FunctionTransformer\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_curve, auc, precision_recall_curve, \n",
    "    PrecisionRecallDisplay, RocCurveDisplay\n",
    ")\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288d42cc-0cd2-403c-b253-fe2792064d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities.ipynb\n",
    "# ======================== DATA CATALOG ========================\n",
    "def generate_data_catalog(\n",
    "    data: pd.DataFrame, \n",
    "    discard_reasons: Optional[Dict[str, str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a generic data catalog for any dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data: Input DataFrame\n",
    "    discard_reasons: Optional dict of {column: reason} for variables to discard\n",
    "    \n",
    "    Returns:\n",
    "    Catalog DataFrame with columns: \n",
    "    ['Variable', 'Type', 'Missing%', 'Unique', 'Discard', 'Reason']\n",
    "    \"\"\"\n",
    "    catalog = []\n",
    "    for col in data.columns:\n",
    "        entry = {\n",
    "            \"Variable\": col,\n",
    "            \"Type\": data[col].dtype,\n",
    "            \"Missing%\": round(data[col].isnull().mean() * 100, 2),\n",
    "            \"Unique\": data[col].nunique(),\n",
    "            \"Discard\": \"Yes\" if (discard_reasons and col in discard_reasons) else \"No\",\n",
    "            \"Reason\": discard_reasons.get(col, \"N/A\") if discard_reasons else \"N/A\"\n",
    "        }\n",
    "        catalog.append(entry)\n",
    "    \n",
    "    return pd.DataFrame(catalog)\n",
    "\n",
    "# ==================== GENERIC PREPROCESSING ====================\n",
    "def build_preprocessing_pipeline(\n",
    "    numerical_strategy: str = 'median',\n",
    "    categorical_strategy: str = 'most_frequent',\n",
    "    scaler: Union[StandardScaler, MinMaxScaler, None] = StandardScaler()\n",
    ") -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Creates generic preprocessing pipeline for any dataset.\n",
    "    \n",
    "    Automatically detects numerical/categorical columns.\n",
    "    \"\"\"\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=numerical_strategy)),\n",
    "        ('scaler', scaler if scaler else 'passthrough')\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=categorical_strategy)),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, make_column_selector(dtype_include=np.number)),\n",
    "            ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# ================== GENERIC VISUALIZATIONS =====================\n",
    "def plot_roc_curve(\n",
    "    y_true: pd.Series, \n",
    "    y_prob: np.ndarray,\n",
    "    ax: Optional[plt.Axes] = None,\n",
    "    title: str = 'ROC Curve'\n",
    ") -> None:\n",
    "    \"\"\"Generic ROC curve plotter.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc).plot(ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "def plot_feature_importance(\n",
    "    model: object,\n",
    "    feature_names: List[str],\n",
    "    top_n: int = 10,\n",
    "    ax: Optional[plt.Axes] = None,\n",
    "    title: str = 'Feature Importance'\n",
    ") -> None:\n",
    "    \"\"\"Generic feature importance visualization.\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importance = np.abs(model.coef_).mean(axis=0)  # Handle multi-class\n",
    "    else:\n",
    "        raise ValueError(\"Model doesn't support feature importance\")\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, ax=ax, palette='viridis')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# ==================== GENERIC MLOPS UTILS ======================\n",
    "def save_artifacts(\n",
    "    model: object,\n",
    "    preprocessor: Optional[ColumnTransformer] = None,\n",
    "    model_path: str = 'model.joblib',\n",
    "    preprocessor_path: str = 'preprocessor.joblib'\n",
    ") -> None:\n",
    "    \"\"\"Generic artifact saver for deployment.\"\"\"\n",
    "    dump(model, model_path)\n",
    "    if preprocessor:\n",
    "        dump(preprocessor, preprocessor_path)\n",
    "    print(f\"Saved artifacts: {model_path}\" + \n",
    "          (f\", {preprocessor_path}\" if preprocessor else \"\"))\n",
    "\n",
    "# ================== GENERIC DATA QUALITY =======================\n",
    "def detect_data_issues(\n",
    "    data: pd.DataFrame,\n",
    "    z_threshold: float = 3,\n",
    "    correlation_threshold: float = 0.9\n",
    ") -> Dict[str, Union[pd.DataFrame, float]]:\n",
    "    \"\"\"\n",
    "    Generic data quality check:\n",
    "    - Missing values\n",
    "    - Duplicates\n",
    "    - Outliers (Z-score)\n",
    "    - High correlations\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Missing values\n",
    "    report['missing_values'] = data.isna().sum()\n",
    "    \n",
    "    # Duplicates\n",
    "    report['duplicate_rows'] = data.duplicated().sum()\n",
    "    \n",
    "    # Outliers\n",
    "    numerical_cols = data.select_dtypes(include=np.number).columns\n",
    "    z_scores = data[numerical_cols].apply(zscore)\n",
    "    report['outliers'] = (np.abs(z_scores) > z_threshold).sum()\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = data[numerical_cols].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    report['high_correlations'] = upper.stack().sort_values(ascending=False)[\n",
    "        upper.stack() > correlation_threshold\n",
    "    ]\n",
    "    \n",
    "    return report\n",
    "\n",
    "# ==================== EXAMPLE USAGE ====================\n",
    "#  if __name__ == \"__main__\":\n",
    "#     # Sample generic usage\n",
    "#     data = pd.DataFrame({\n",
    "#         'age': [25, 30, None, 40],\n",
    "#         'gender': ['M', 'F', 'M', None],\n",
    "#         'readmitted': [1, 0, 1, 0]\n",
    "#     })\n",
    "    \n",
    "    # Generate catalog\n",
    "#     catalog = generate_data_catalog(data, {'gender': 'High missingness'})\n",
    "#     print(\"Data Catalog:\")\n",
    "#     print(catalog)\n",
    "    \n",
    "    # Detect issues\n",
    "#     print(\"\\nData Issues Report:\")\n",
    "#     print(detect_data_issues(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff481f-2125-42ec-bdf3-3a48c9c3ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
